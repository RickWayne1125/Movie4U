{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe3391c-90f4-49e8-b92b-98c5cc09a392",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-06T05:43:56.785373Z",
     "iopub.status.busy": "2022-01-06T05:43:56.784847Z",
     "iopub.status.idle": "2022-01-06T05:44:01.043326Z",
     "shell.execute_reply": "2022-01-06T05:44:01.042136Z",
     "shell.execute_reply.started": "2022-01-06T05:43:56.785231Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/01/05 23:43:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/01/05 23:43:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/01/05 23:43:59 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/01/05 23:43:59 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ScalableMovieRecommendations\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4612cfa2-bfa3-4043-8b3f-14875584c263",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-06T05:44:41.297843Z",
     "iopub.status.busy": "2022-01-06T05:44:41.297367Z",
     "iopub.status.idle": "2022-01-06T05:44:42.952630Z",
     "shell.execute_reply": "2022-01-06T05:44:42.951582Z",
     "shell.execute_reply.started": "2022-01-06T05:44:41.297792Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "movies_df_schema = \"ID integer, title string\"\n",
    "\n",
    "moviesDF = spark.read.csv(\"../datasets/ml-latest/movies.csv\", header=True, schema=movies_df_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9600ff2d-00ac-4271-bb92-9abede851ea9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-06T05:45:14.051903Z",
     "iopub.status.busy": "2022-01-06T05:45:14.051445Z",
     "iopub.status.idle": "2022-01-06T05:45:15.938612Z",
     "shell.execute_reply": "2022-01-06T05:45:15.937533Z",
     "shell.execute_reply.started": "2022-01-06T05:45:14.051850Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: int, title: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| ID|               title|\n",
      "+---+--------------------+\n",
      "|  1|    Toy Story (1995)|\n",
      "|  2|      Jumanji (1995)|\n",
      "|  3|Grumpier Old Men ...|\n",
      "|  4|Waiting to Exhale...|\n",
      "|  5|Father of the Bri...|\n",
      "|  6|         Heat (1995)|\n",
      "|  7|      Sabrina (1995)|\n",
      "|  8| Tom and Huck (1995)|\n",
      "|  9| Sudden Death (1995)|\n",
      "| 10|    GoldenEye (1995)|\n",
      "| 11|American Presiden...|\n",
      "| 12|Dracula: Dead and...|\n",
      "| 13|        Balto (1995)|\n",
      "| 14|        Nixon (1995)|\n",
      "| 15|Cutthroat Island ...|\n",
      "| 16|       Casino (1995)|\n",
      "| 17|Sense and Sensibi...|\n",
      "| 18|   Four Rooms (1995)|\n",
      "| 19|Ace Ventura: When...|\n",
      "| 20|  Money Train (1995)|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/05 23:45:15 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 3, schema size: 2\n",
      "CSV file: file:///home/proxslick/spark-movie-lens/datasets/ml-latest/movies.csv\n"
     ]
    }
   ],
   "source": [
    "display(moviesDF)\n",
    "moviesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44412ec2-277d-4fcc-b802-9eca92ebff69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-06T05:45:59.056375Z",
     "iopub.status.busy": "2022-01-06T05:45:59.055907Z",
     "iopub.status.idle": "2022-01-06T05:46:22.395216Z",
     "shell.execute_reply": "2022-01-06T05:46:22.393936Z",
     "shell.execute_reply.started": "2022-01-06T05:45:59.056322Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/05 23:45:59 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 4, schema size: 3\n",
      "CSV file: file:///home/proxslick/spark-movie-lens/datasets/ml-latest/ratings.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 27753444 ratings and 58098 movies in the datasets.\n"
     ]
    }
   ],
   "source": [
    "ratings_df_schema = \"userId integer, movieId integer, rating float\"\n",
    "\n",
    "ratingsDF = spark.read.csv(\"../datasets/ml-latest/ratings.csv\", header=True, schema=ratings_df_schema).cache()\n",
    "\n",
    "ratingsCount = ratingsDF.count()\n",
    "moviesCount = moviesDF.count()\n",
    "\n",
    "print('There are {0} ratings and {1} movies in the datasets.'.format(ratingsCount, moviesCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee584d8b-08c7-474e-90bc-f38f617dc501",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-06T05:47:32.421931Z",
     "iopub.status.busy": "2022-01-06T05:47:32.421466Z",
     "iopub.status.idle": "2022-01-06T05:47:32.621910Z",
     "shell.execute_reply": "2022-01-06T05:47:32.620713Z",
     "shell.execute_reply.started": "2022-01-06T05:47:32.421879Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[userId: int, movieId: int, rating: float]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|     1|    307|   3.5|\n",
      "|     1|    481|   3.5|\n",
      "|     1|   1091|   1.5|\n",
      "|     1|   1257|   4.5|\n",
      "|     1|   1449|   4.5|\n",
      "|     1|   1590|   2.5|\n",
      "|     1|   1591|   1.5|\n",
      "|     1|   2134|   4.5|\n",
      "|     1|   2478|   4.0|\n",
      "|     1|   2840|   3.0|\n",
      "|     1|   2986|   2.5|\n",
      "|     1|   3020|   4.0|\n",
      "|     1|   3424|   4.5|\n",
      "|     1|   3698|   3.5|\n",
      "|     1|   3826|   2.0|\n",
      "|     1|   3893|   3.5|\n",
      "|     2|    170|   3.5|\n",
      "|     2|    849|   3.5|\n",
      "|     2|   1186|   3.5|\n",
      "|     2|   1235|   3.0|\n",
      "+------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display(ratingsDF)\n",
    "ratingsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b24b7ce0-abfb-438f-b726-14c7003cba21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-06T05:48:23.538806Z",
     "iopub.status.busy": "2022-01-06T05:48:23.538346Z",
     "iopub.status.idle": "2022-01-06T05:48:26.408455Z",
     "shell.execute_reply": "2022-01-06T05:48:26.407478Z",
     "shell.execute_reply.started": "2022-01-06T05:48:23.538754Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:===================================================>    (22 + 2) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 22204203, test: 5549241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "seed=42\n",
    "(trainingDF, testDF) = ratingsDF.randomSplit([0.8, 0.2], seed=seed)\n",
    "\n",
    "print('Training: {0}, test: {1}'.format(trainingDF.count(), testDF.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b6c87ab-e7ff-45bf-b0c1-99a28ba62e89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-06T05:48:43.192161Z",
     "iopub.status.busy": "2022-01-06T05:48:43.191702Z",
     "iopub.status.idle": "2022-01-06T05:48:57.656165Z",
     "shell.execute_reply": "2022-01-06T05:48:57.655238Z",
     "shell.execute_reply.started": "2022-01-06T05:48:43.192108Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/05 23:48:51 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/01/05 23:48:51 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "22/01/05 23:48:52 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"16\")\n",
    "\n",
    "als = (ALS()\n",
    "       .setUserCol(\"userId\")\n",
    "       .setItemCol(\"movieId\")\n",
    "       .setRatingCol(\"rating\")\n",
    "       .setPredictionCol(\"predictions\")\n",
    "       .setMaxIter(2)\n",
    "       .setSeed(seed)\n",
    "       .setRegParam(0.1)\n",
    "       .setColdStartStrategy(\"drop\")\n",
    "       .setRank(12))\n",
    "\n",
    "alsModel = als.fit(trainingDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b80f28e-2ab7-4fbe-b16e-6622ece9d24a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-06T05:49:31.692830Z",
     "iopub.status.busy": "2022-01-06T05:49:31.692373Z",
     "iopub.status.idle": "2022-01-06T05:49:36.879646Z",
     "shell.execute_reply": "2022-01-06T05:49:36.878383Z",
     "shell.execute_reply.started": "2022-01-06T05:49:31.692778Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:================================>                        (9 + 7) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model had a MSE on the test set of 0.7466344590904963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "regEval = RegressionEvaluator(predictionCol=\"predictions\", labelCol=\"rating\", metricName=\"mse\")\n",
    "\n",
    "predictedTestDF = alsModel.transform(testDF)\n",
    "\n",
    "testMse = regEval.evaluate(predictedTestDF)\n",
    "\n",
    "print('The model had a MSE on the test set of {0}'.format(testMse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b401c5ee-6bea-428b-9d3b-09340ef17e8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-06T05:50:04.825293Z",
     "iopub.status.busy": "2022-01-06T05:50:04.824667Z",
     "iopub.status.idle": "2022-01-06T05:50:04.946924Z",
     "shell.execute_reply": "2022-01-06T05:50:04.945532Z",
     "shell.execute_reply.started": "2022-01-06T05:50:04.825237Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "userFactors = alsModel.userFactors.selectExpr(\"id as userId\", \"features as uFeatures\")\n",
    "itemFactors = alsModel.itemFactors.selectExpr(\"id as movieId\", \"features as iFeatures\")\n",
    "joinedTrainDF = trainingDF.join(itemFactors, on=\"movieId\").join(userFactors, on=\"userId\")\n",
    "joinedTestDF = testDF.join(itemFactors, on=\"movieId\").join(userFactors, on=\"userId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2da778a3-9b9b-4b9e-a39f-8abf3d85061c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-06T05:50:48.825130Z",
     "iopub.status.busy": "2022-01-06T05:50:48.824531Z",
     "iopub.status.idle": "2022-01-06T05:50:48.920781Z",
     "shell.execute_reply": "2022-01-06T05:50:48.919611Z",
     "shell.execute_reply.started": "2022-01-06T05:50:48.825046Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def concat_arrays(*args):\n",
    "    return list(chain(*args))\n",
    "    \n",
    "concat_arrays_udf = udf(concat_arrays, ArrayType(FloatType()))\n",
    "\n",
    "concatTrainDF = (joinedTrainDF\n",
    "                 .select('userId', 'movieId', concat_arrays_udf(col(\"iFeatures\"), col(\"uFeatures\")).alias(\"features\"), \"rating\"))\n",
    "concatTestDF = (joinedTestDF\n",
    "                .select('userId', 'movieId', concat_arrays_udf(col(\"iFeatures\"), col(\"uFeatures\")).alias(\"features\"), \"rating\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89f7b91-4848-44d1-bb4a-6268181ea6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import horovod.tensorflow as hvd\n",
    "\n",
    "tf.set_random_seed(seed=40)\n",
    "\n",
    "def model_fn(features, labels, mode, params):\n",
    "    print(\"HVD Size: \", hvd.size())\n",
    "    features_with_shape = tf.reshape(features[\"features\"], [-1, 24]) # Explicitly specify dimensions\n",
    "    \n",
    "    hidden_layer1 = tf.layers.dense(inputs=features_with_shape, units=params[\"hidden_layer1\"], activation=tf.nn.relu)\n",
    "    hidden_layer2 = tf.layers.dense(inputs=hidden_layer1, units=params[\"hidden_layer2\"], activation=tf.nn.relu)\n",
    "    predictions = tf.squeeze(tf.layers.dense(inputs=hidden_layer2, units=1, activation=None), axis=-1)\n",
    "    \n",
    "    # If the estimator is running in PREDICT mode, we can stop building our model graph here and simply return\n",
    "    # our model's inference outputs\n",
    "    serving_key = tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY\n",
    "    export_outputs = {serving_key: tf.estimator.export.PredictOutput({\"predictions\": predictions})}\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions, export_outputs=export_outputs)\n",
    "      \n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.mean_squared_error(labels, predictions)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=params[\"learning_rate\"])\n",
    "        optimizer = hvd.DistributedOptimizer(optimizer)\n",
    "        \n",
    "        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op,\n",
    "                                          export_outputs=export_outputs)\n",
    "    # If running in EVAL mode, add model evaluation metrics (accuracy) to our EstimatorSpec so that\n",
    "    # they're logged when model evaluation runs\n",
    "    eval_metric_ops = {\"rmse\": tf.metrics.root_mean_squared_error(labels=labels, predictions=predictions)}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops, export_outputs=export_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135e2bd6-75a1-4d79-a947-4a4ccb499723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "trainValDF = concatTrainDF.withColumn(\"isVal\", when(rand() > 0.8, True).otherwise(False))\n",
    "\n",
    "model_dir = \"/tmp/horovodDemo/\" + str(int(time.time()))\n",
    "print(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17302c94-df91-4541-adcf-9b1021ea276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkdl.estimators.horovod_estimator.estimator import HorovodEstimator\n",
    "\n",
    "est = HorovodEstimator(modelFn=model_fn,\n",
    "                       featureMapping={\"features\":\"features\"},\n",
    "                       modelDir=model_dir,\n",
    "                       labelCol=\"rating\",\n",
    "                       batchSize=128,\n",
    "                       maxSteps=20000,\n",
    "                       isValidationCol=\"isVal\",  \n",
    "                       modelFnParams={\"hidden_layer1\": 30, \"hidden_layer2\": 20, \"learning_rate\": 0.0001},\n",
    "                       saveCheckpointsSecs=30)\n",
    "transformer = est.fit(trainValDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ee90b1-1380-4be5-9e88-67550ca806fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(transformer.transform(concatTestDF).select(\"userId\", \"movieId\", \"predictions\", \"rating\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c587fc51-6eb3-436e-81ee-aa31f3fe9122",
   "metadata": {},
   "outputs": [],
   "source": [
    "testMse = regEval.evaluate(transformer.transform(concatTestDF))\n",
    "\n",
    "print('The model had a MSE on the test set of {0}'.format(testMse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
